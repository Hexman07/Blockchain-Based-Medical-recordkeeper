{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl198Zv6v6N1jnP4DoUZ+4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hexman07/Blockchain-Based-Medical-recordkeeper/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFTecduAqiWE",
        "outputId": "c3747349-d427-42e8-ead6-e998368ed0ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JCL-IP_qqibr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/LCS2-IIITD/CACN-EMNLP-2023.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbDwqQybqioL",
        "outputId": "c311d4f6-1ce1-4f6a-8776-d506d0b39802"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CACN-EMNLP-2023'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 27 (delta 9), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (27/27), 18.05 KiB | 1.64 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j_5TR4gcqiyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"cacn\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "Author: Megha Sundriyal\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1x0CL7qbcfBiBiQ9jGBP85c3Y45UOQDeX\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import openai\n",
        "import logging\n",
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "class CACN:\n",
        "    def __init__(self, api_key, prompt_file, data_file, output_file):\n",
        "        self.api_key = api_key\n",
        "        self.prompt = self.get_in_context_examples(prompt_file)\n",
        "        self.data = self.get_data(data_file)\n",
        "\n",
        "    def get_in_context_examples(self, prompt_file):\n",
        "        \"\"\"\n",
        "        Read the In-context examples from the given file\n",
        "\n",
        "            Args: file path\n",
        "            Returns: cleaned in-context examples to append with prompt.\n",
        "        \"\"\"\n",
        "        # Read the prompt from the text file and save as prompt\n",
        "        with open(prompt_file, 'r') as file:\n",
        "            prompt = file.read()\n",
        "            prompt = re.sub(r'[^a-zA-Z0-9.:?\\s]', '', prompt)\n",
        "            prompt = prompt.replace('\\t', ' ').replace('\\n', ' ')\n",
        "\n",
        "        return prompt\n",
        "\n",
        "\n",
        "    def decontract(self, text):\n",
        "        \"\"\"\n",
        "        Decontract the contracted words in the given text.\n",
        "        Args:\n",
        "            text (str): Text containing contracted words.\n",
        "        Returns:\n",
        "            str: Text with contracted words expanded.\n",
        "        Example:\n",
        "            >>> decontract(\"I can't go. It's raining.\")\n",
        "            \"I cannot go. It is raining.\"\n",
        "        \"\"\"\n",
        "        contractions_dict = {}\n",
        "        with open('/content/CACN-EMNLP-2023/replacements.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                key, value = line.strip().split(': ')\n",
        "                contractions_dict[key] = value\n",
        "\n",
        "        for contraction, expansion in contractions_dict.items():\n",
        "          pattern = re.compile(contraction, re.IGNORECASE)\n",
        "          text = re.sub(pattern, expansion, text)\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def clean_post(self, post):\n",
        "        # remove links\n",
        "        post = re.sub(r\"http:\\S+\", \"\", post)\n",
        "        # remove special characters\n",
        "        post = re.sub(r\"[\\(\\)#@!\\^\\\\\\/\\+><]\", \"\", post)\n",
        "        # remove extra white spaces\n",
        "        post = re.sub(r\"\\s+\", \" \", post)\n",
        "        # lower case\n",
        "        post = post.lower()\n",
        "\n",
        "        return post\n",
        "\n",
        "\n",
        "    def get_data(self, data_file):\n",
        "        \"\"\"\n",
        "        Read the data and resturn preprocssed posts.\n",
        "\n",
        "            Args: File path\n",
        "            Returns: Dataframe\n",
        "\n",
        "        Example:\n",
        "            >>> decontract(\"I can't go. It's too late.\")\n",
        "            \"I cannot go. It is too late.\"\n",
        "        \"\"\"\n",
        "\n",
        "        df =  pd.read_csv(data_file)\n",
        "\n",
        "        # preprocess posts\n",
        "        df['clean post']= df['Social Media Post'].apply(self.decontract)\n",
        "        df['clean post'] = df['clean post'].apply(self.clean_post)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def extract_claim(self, sentence):\n",
        "        \"\"\"\n",
        "        Extract normalized claim from the response generated by the model\n",
        "\n",
        "        Args:\n",
        "            sentence (str): The sentence generetaed by gpt\n",
        "\n",
        "        Returns:\n",
        "            str: Normalized claim\n",
        "\n",
        "        Example:\n",
        "            >>> extract_claim(\"The post claims that Thailand will ban Pfizer vaccines after a Thai princess falls into a coma following a booster jab. This claim is verifiable through various reports and has a huge social impact. Thus, the central claim here is Thailand will ban Pfizer vaccines after a Thai princess falls into a coma following a booster jab.\")\n",
        "            \"Thailand will ban Pfizer vaccines after a Thai princess falls into a coma following a booster jab.\"\n",
        "        \"\"\"\n",
        "\n",
        "        sentence = sentence.replace(\"U.S.\", \"US\")\n",
        "        sentence = sentence.replace(\"Dr.\", \"Dr\")\n",
        "        sentence = sentence.replace(\"Ph.D.\", \"PhD\")\n",
        "\n",
        "        sentence = nltk.sent_tokenize(sentence)\n",
        "        claim = sentence[-1].strip()   #get last sentence, which contains the central claim from the response.\n",
        "        # print(last_sentence)\n",
        "\n",
        "        pattern = r'(the central claim is|the normalized checkworthy claim is|the crucial checkworthy claim is|claim should be|claim here is|normalized checkworthy claim should be|central checkworthy claim is|normalized checkworthy claim should be:|he central claim in the post is|to be fact-checked here is)(?: that)?(.*?)[\\.\\n]'\n",
        "        matches = re.search(pattern, claim, flags=re.IGNORECASE)\n",
        "        if matches:\n",
        "            claim = matches.group(2).strip()\n",
        "            claim = claim.replace(\":\", \"\")\n",
        "            return claim\n",
        "        return claim\n",
        "\n",
        "    def generate_claims(self):\n",
        "        openai.api_key = self.api_key\n",
        "        SUMM_MAX_LENGTH = 120\n",
        "        MAX_TOKEN_LIMIT = 4096\n",
        "\n",
        "        with open(output_file, 'a', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(['Social Media post','CACN Normalized Claim','Gold Normalized Claim'])  #create a new file and write headers\n",
        "\n",
        "\n",
        "            for instance in range(len(self.data)):\n",
        "                text = self.data['clean post'].iloc[instance]\n",
        "                prompt_text = f\"{self.prompt} Identify the central claim in the given post: {text} \\n Let's think step by step.\"\n",
        "\n",
        "                if len(prompt_text) > MAX_TOKEN_LIMIT:\n",
        "                    text = text[:MAX_TOKEN_LIMIT]\n",
        "                    prompt_text = f\"{self.prompt} Identify the central claim in the given post: {text} \\n Let's think step by step.\"\n",
        "\n",
        "                response = openai.Completion.create(\n",
        "                    engine=\"gpt-3.5-turbo-instruct\",\n",
        "                    prompt=prompt_text,\n",
        "                    temperature=0.6,\n",
        "                    max_tokens=SUMM_MAX_LENGTH,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0.1,\n",
        "                    stop=None\n",
        "                )\n",
        "\n",
        "                gpt_summary = response[\"choices\"][0][\"text\"].strip()\n",
        "                normalized_claim = self.extract_claim(gpt_summary)\n",
        "                row = [self.data.iloc[instance]['Social Media Post'], normalized_claim, self.data.iloc[instance]['Normalized Claim']]\n",
        "                print(normalized_claim)\n",
        "                writer.writerow(row)\n",
        "                print(row)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    api_key = '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'\n",
        "    prompt_file = '/content/CACN-EMNLP-2023/prompt.txt'\n",
        "    data_file = '/content/CACN-EMNLP-2023/CLAN-samples.csv'\n",
        "    output_file = '/content/CACN-EMNLP-2023/output.csv'\n",
        "\n",
        "    claim_extractor = CACN(api_key, prompt_file, data_file, output_file)\n",
        "    claim_extractor.generate_claims()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "mxnKMlnpqi3y",
        "outputId": "2d8ff748-309d-4135-b667-89cf2db71ce0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abortion is never medically necessary to save the life of the mother\n",
            "['If you are experiencing pregnancy complications and your doctor tells you that abortion is necessary, find a new doctor. You and your baby deserve better. Abortion is never medically necessary to save the life of the mother.', 'Abortion is never medically necessary to save the life of the mother', 'abortions are never medically necessary']\n",
            "This claim can be verified through data and statistics on medication prices and the impact it has on American families.\n",
            "[\"Americans are traveling to Canada to pay 12X less for insulin than they would here in the US. That's unacceptable.\\r\\n \\r\\n Itâ€™s time American families are treated fairly & pay a fair price for life-saving meds. That's why I'm fighting for a common-sense solution: http://bit.ly/SRS_TDPA\", 'This claim can be verified through data and statistics on medication prices and the impact it has on American families.', '\"Americans are traveling to Canada to pay 12 times less for insulin than they would here in the US.\"  ']\n",
            "Dandelion root can cure cancer and has various health benefits\n",
            "['DID YOU KNOW?\\r\\nNatural cures89\\r\\nDandelion root is able to kill 98% of cancer cells within 48 hours. Not only that, but it acts as a powerful anti- inflammatory, immune booster, antioxidant and organ detoxifier', 'Dandelion root can cure cancer and has various health benefits', 'Dandelion root is able to kill 98% of cancer cells within 48 hours.']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Rate limit reached for gpt-3.5-turbo-instruct in organization org-gDrpGVl5hDKiFBr5ACqIVEAJ on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a6065d93ca5c>\u001b[0m in \u001b[0;36m<cell line: 166>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mclaim_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCACN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mclaim_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_claims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-a6065d93ca5c>\u001b[0m in \u001b[0;36mgenerate_claims\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mprompt_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{self.prompt} Identify the central claim in the given post: {text} \\n Let's think step by step.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 response = openai.Completion.create(\n\u001b[0m\u001b[1;32m    150\u001b[0m                     \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo-instruct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for gpt-3.5-turbo-instruct in organization org-gDrpGVl5hDKiFBr5ACqIVEAJ on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing."
          ]
        }
      ]
    }
  ]
}